{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8925f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import bernoulli\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd993831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define problem parameters\n",
    "N = 1000  # num data points arriving per exp\n",
    "J = 50    # num exp\n",
    "numfeat = 50  # number of features\n",
    "C = 2 # number of classes\n",
    "total_seed = 10 # total number of seeds \n",
    "\n",
    "# model learns a feature v when there are at least \"gamma\" data points\n",
    "gamma = 50\n",
    "\n",
    "# a data point has zero gradient when there are \"tau\" features in the data point that are already learned by the model.\n",
    "tau = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39475ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct train data\n",
    "def get_train_data(N, J, C, numfeat, seed=0):\n",
    "    N_c = int(N * J * 1/C)\n",
    "    np.random.seed(seed)\n",
    "    X = np.zeros((C, numfeat, N_c))\n",
    "\n",
    "    p = [] # empty prob list\n",
    "    for c in range(C):\n",
    "        prob = np.sort(0.2 * np.random.rand(numfeat))[::-1]   # sampled from 0~0.2, sorted\n",
    "        p.append(prob)\n",
    "        for i in range(N_c):\n",
    "            X[c, :, i] = bernoulli.rvs(prob)\n",
    "\n",
    "    return X, p\n",
    "\n",
    "def get_sort_indexes(arr):\n",
    "    indexed_arr = list(enumerate(arr))\n",
    "    sorted_arr = sorted(indexed_arr, key=lambda x: x[1], reverse=True)\n",
    "    sort_indexes = [item[0] for item in sorted_arr]\n",
    "    \n",
    "    return sort_indexes\n",
    "\n",
    "# construct test data\n",
    "def get_test_data(data_num, C, numfeat, prob_lst, seed=0):\n",
    "    np.random.seed(seed * 100 + 100)\n",
    "    \n",
    "    X = np.zeros((C, numfeat, data_num//C))\n",
    "    for c in range(C):\n",
    "        for i in range(int(data_num * 1/C)):\n",
    "            X[c, :, i] = bernoulli.rvs(prob_lst[c])\n",
    "\n",
    "    return X\n",
    "\n",
    "# Calculate Test accuracy\n",
    "def cal_testacc(test_data, learn_lst, C, data_num=10000, tau=2):\n",
    "    acc = 0\n",
    "    for c in range(C):\n",
    "        for data in test_data[c].T:\n",
    "            if np.sum(data * learn_lst[c]) >= tau:\n",
    "                acc += 1\n",
    "            else:\n",
    "                acc += bernoulli.rvs(1/C)\n",
    "    acc /= data_num\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5dba7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Learn_F = np.zeros((total_seed, J, C, numfeat, 3))  # features learned for the 3 methods\n",
    "Learn_X = np.zeros((total_seed, J, C, int(N * (J+1) * 1/C), 3))    # data points learned for the 3 methods\n",
    "Memorize_X = np.zeros((total_seed, J, C, int(N * (J+1) * 1/C), 3)) # data points memorized for the 3 methods\n",
    "Nzero_X = np.zeros((total_seed, J, 3))           # number of non-zero gradient data points for the 3 methods\n",
    "prob_lst = np.zeros((total_seed, C, numfeat)) # save prob_lst to sample test_data from same distribution\n",
    "\n",
    "for seed in tqdm.tqdm(range(total_seed)):\n",
    "    # Initialize settings\n",
    "    X, p = get_train_data(N, J, C, numfeat, seed)\n",
    "    prob_lst[seed] = p\n",
    "\n",
    "    # Method 1: rand training, sequentially.\n",
    "    for j in range(J):\n",
    "        Nzero_X[seed, j, 0] = N * (j+1)\n",
    "        N_c = int(N * (j+1) * 1/C)\n",
    "        nzero_idx = np.arange(N_c)\n",
    "        nzero_idx = np.tile(nzero_idx, (C, 1)).tolist()\n",
    "\n",
    "        # Sequential train\n",
    "        while True:\n",
    "            fcount = np.zeros((C, numfeat))\n",
    "            for c in range(C):\n",
    "                fcount[c] = np.sum(X[c, :, nzero_idx[c]], axis=0)\n",
    "\n",
    "            # sort feature indexes by descending order to have a list of a tuple, [(c, idx), ...]\n",
    "            sort_indexes = [(idx // numfeat, idx % numfeat) for idx in get_sort_indexes(fcount.flatten())]\n",
    "\n",
    "            # Break if all features are learned\n",
    "            nolearn_idx = (Learn_F[seed, j, :, :, 0] == 0)\n",
    "            if all(fcount[nolearn_idx] < gamma):\n",
    "                break\n",
    "            for idx in sort_indexes:\n",
    "                # Continue if already learned\n",
    "                if Learn_F[seed, j, idx[0], idx[1], 0] == 1:\n",
    "                    continue\n",
    "\n",
    "                # Learn if # of occurence is sufficiently large\n",
    "                if fcount[idx] >= gamma:\n",
    "                    Learn_F[seed, j, idx[0], idx[1], 0] = 1\n",
    "\n",
    "                    # Make zero gradient\n",
    "                    for c in range(C):\n",
    "                        for i in nzero_idx[c]:\n",
    "                            idx = np.where(X[c, :, i] == 1)[0] \n",
    "                            if np.sum(Learn_F[seed, j, c, idx, 0]) >= tau:\n",
    "                                Learn_X[seed, j, c, i, 0] = 1\n",
    "\n",
    "                        learn_idx = np.where(Learn_X[seed, j, c, :, 0] == 1)[0]\n",
    "                        nzero_idx[c] = list(set(nzero_idx[c]) - set(learn_idx))\n",
    "                    break #break for loop since we need to calculate fcount again if one feature is learned.\n",
    "        for c in range(C):\n",
    "            Memorize_X[seed, j, c, nzero_idx[c], 0] = 1\n",
    "\n",
    "        print(f'\\nX at start of the {j+1}th experiment (Random):', int(Nzero_X[seed, j, 0]))\n",
    "        print(f'Learned Features at {j+1}th experiment (Random):', np.sum(Learn_F[seed, j, :, :, 0]==1))\n",
    "        print(f'Learned X at {j+1}th experiment (Random):', np.sum(Learn_X[seed, j, :, :, 0]==1))\n",
    "        print(f'Memorized X at {j+1}th experiment (Random):', np.sum(Memorize_X[seed, j, :, :, 0]==1))        \n",
    "\n",
    "\n",
    "    # Method 2: warm training, sequentially.\n",
    "    for j in range(J):\n",
    "        N_c = int(N * (j+1) * 1/C)\n",
    "        nzero_idx = np.arange(int(N * j * 1/C), N_c)\n",
    "        nzero_idx = np.tile(nzero_idx, (C, 1)).tolist()\n",
    "        # convey previous experiments' information\n",
    "        if j >= 1:\n",
    "            Learn_F[seed, j, :, :, 1] = Learn_F[seed, j-1, :, :, 1]\n",
    "            Learn_X[seed, j, :, :, 1] = Learn_X[seed, j-1, :, :, 1]\n",
    "            Memorize_X[seed, j, :, :, 1] = Memorize_X[seed, j-1, :, :, 1]\n",
    "\n",
    "        # check zero gradient for newly added data\n",
    "        for c in range(C):\n",
    "            for i in nzero_idx[c]:\n",
    "                idx = np.where(X[c, :, i] == 1)[0]\n",
    "                if np.sum(Learn_F[seed, j, c, idx, 1]) >= tau:\n",
    "                    Learn_X[seed, j, c, i, 1] = 1\n",
    "\n",
    "            learn_idx = np.where(Learn_X[seed, j, c, :, 1] == 1)[0]\n",
    "            nzero_idx[c] = list(set(nzero_idx[c]) - set(learn_idx))\n",
    "        Nzero_X[seed, j, 1] = sum([len(nzero_idx[c]) for c in range(C)])\n",
    "\n",
    "        # Start training\n",
    "        while True:\n",
    "            fcount = np.zeros((C, numfeat))\n",
    "            for c in range(C):\n",
    "                fcount[c] = np.sum(X[c, :, nzero_idx[c]], axis=0)\n",
    "            sort_indexes = [(idx // numfeat, idx % numfeat) for idx in get_sort_indexes(fcount.flatten())]\n",
    "\n",
    "            # Break if all features are learned\n",
    "            nolearn_idx = (Learn_F[seed, j, :, :, 1] == 0)\n",
    "            if all(fcount[nolearn_idx] < gamma):\n",
    "                break\n",
    "            for idx in sort_indexes:\n",
    "                # Continue if already learned\n",
    "                if Learn_F[seed, j, idx[0], idx[1], 1] == 1:\n",
    "                    continue\n",
    "\n",
    "                # Learn if # of occurence is sufficiently large    \n",
    "                if fcount[idx] >= gamma:\n",
    "                    Learn_F[seed, j, idx[0], idx[1], 1] = 1\n",
    "\n",
    "                    # Make zero gradient\n",
    "                    for c in range(C):\n",
    "                        for i in nzero_idx[c]:\n",
    "                            idx = np.where(X[c, :, i] == 1)[0] \n",
    "                            if np.sum(Learn_F[seed, j, c, idx, 1]) >= tau:\n",
    "                                Learn_X[seed, j, c, i, 1] = 1\n",
    "\n",
    "                        learn_idx = np.where(Learn_X[seed, j, c, :, 1] == 1)[0]\n",
    "                        nzero_idx[c] = list(set(nzero_idx[c]) - set(learn_idx))\n",
    "                    break #break for loop since we need to calculate fcount again if one feature is learned.\n",
    "\n",
    "        for c in range(C):\n",
    "            Memorize_X[seed, j, c, nzero_idx[c], 1] = 1\n",
    "\n",
    "        print(f'\\nX at start of the {j+1}th experiment (Warm):', int(Nzero_X[seed, j, 1]))\n",
    "        print(f'Learned Features at {j+1}th experiment (Warm):', np.sum(Learn_F[seed, j, :, :, 1]==1))\n",
    "        print(f'Learned X at {j+1}th experiment (Warm):', np.sum(Learn_X[seed, j, :, :, 1]==1))\n",
    "        print(f'Memorized X at {j+1}th experiment (Warm):', np.sum(Memorize_X[seed, j, :, :, 1]==1))        \n",
    "\n",
    "    # Method 3: forget noise (ideal)\n",
    "    for j in range(J):\n",
    "        Nzero_X[seed, j, 0] = N * (j+1)\n",
    "        N_c = int(N * (j+1) * 1/C)\n",
    "        nzero_idx = np.arange(N_c)\n",
    "        nzero_idx = np.tile(nzero_idx, (C, 1)).tolist()\n",
    "\n",
    "        # convey only properly learned features/data\n",
    "        if j >= 1:\n",
    "            Learn_F[seed, j, :, :, 2] = Learn_F[seed, j-1, :, :, 2]\n",
    "            Learn_X[seed, j, :, :, 2] = Learn_X[seed, j-1, :, :, 2]\n",
    "\n",
    "        # check zero gradient for newly added data\n",
    "        for c in range(C):\n",
    "            for i in nzero_idx[c]:\n",
    "                idx = np.where(X[c, :, i] == 1)[0]\n",
    "                if np.sum(Learn_F[seed, j, c, idx, 2]) >= tau:\n",
    "                    Learn_X[seed, j, c, i, 2] = 1\n",
    "\n",
    "            learn_idx = np.where(Learn_X[seed, j, c, :, 2] == 1)[0]\n",
    "            nzero_idx[c] = list(set(nzero_idx[c]) - set(learn_idx))\n",
    "        Nzero_X[seed, j, 2] = sum([len(nzero_idx[c]) for c in range(C)])\n",
    "\n",
    "\n",
    "        # Start training\n",
    "        while True:\n",
    "            fcount = np.zeros((C, numfeat))\n",
    "            for c in range(C):\n",
    "                fcount[c] = np.sum(X[c, :, nzero_idx[c]], axis=0)\n",
    "            sort_indexes = [(idx // numfeat, idx % numfeat) for idx in get_sort_indexes(fcount.flatten())]\n",
    "\n",
    "            # Break if all features are learned\n",
    "            nolearn_idx = (Learn_F[seed, j, :, :, 2] == 0)\n",
    "            if all(fcount[nolearn_idx] < gamma):\n",
    "                break\n",
    "            for idx in sort_indexes:\n",
    "                # Continue if already learned\n",
    "                if Learn_F[seed, j, idx[0], idx[1], 2] == 1:\n",
    "                    continue\n",
    "\n",
    "                # Learn if # of occurence is sufficiently large    \n",
    "                if fcount[idx] >= gamma:\n",
    "                    Learn_F[seed, j, idx[0], idx[1], 2] = 1\n",
    "\n",
    "                    # Make zero gradient\n",
    "                    for c in range(C):\n",
    "                        for i in nzero_idx[c]:\n",
    "                            idx = np.where(X[c, :, i] == 1)[0] \n",
    "                            if np.sum(Learn_F[seed, j, c, idx, 2]) >= tau:\n",
    "                                Learn_X[seed, j, c, i, 2] = 1\n",
    "\n",
    "                        learn_idx = np.where(Learn_X[seed, j, c, :, 2] == 1)[0]\n",
    "                        nzero_idx[c] = list(set(nzero_idx[c]) - set(learn_idx))\n",
    "                    break #break for loop since we need to calculate fcount again if one feature is learned.\n",
    "\n",
    "        for c in range(C):\n",
    "            Memorize_X[seed, j, c, nzero_idx[c], 2] = 1\n",
    "\n",
    "        print(f'\\nX at start of the {j+1}th experiment (Ideal):', int(Nzero_X[seed, j, 2]))\n",
    "        print(f'Learned Features at {j+1}th experiment (Ideal):', np.sum(Learn_F[seed, j, :, :, 2]==1))\n",
    "        print(f'Learned X at {j+1}th experiment (Ideal):', np.sum(Learn_X[seed, j, :, :, 2]==1))\n",
    "        print(f'Memorized X at {j+1}th experiment (Ideal):', np.sum(Memorize_X[seed, j, :, :, 2]==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b084b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate test accuracy for each method\n",
    "\n",
    "test_acc = np.zeros((total_seed, J, 3))\n",
    "for seed in tqdm.tqdm(range(total_seed)):\n",
    "    test_data = get_test_data(10000, C, numfeat, prob_lst[seed], C)\n",
    "    for method in range(3):\n",
    "        for j in range(J):\n",
    "            test_acc[seed, j, method] = cal_testacc(test_data, Learn_F[seed, j, :, :, method], C, 10000, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Figure\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "random_nx = np.mean([Nzero_X[i, :, 0] for i in range(total_seed)], axis=0)\n",
    "warm_nx = np.mean([Nzero_X[i, :, 1] for i in range(total_seed)], axis=0)\n",
    "ideal_nx = np.mean([Nzero_X[i, :, 2] for i in range(total_seed)], axis=0)\n",
    "\n",
    "random_nx_std = np.std([Nzero_X[i, :, 0] for i in range(total_seed)], axis=0)\n",
    "warm_nx_std = np.std([Nzero_X[i, :, 1] for i in range(total_seed)], axis=0)\n",
    "ideal_nx_std = np.std([Nzero_X[i, :, 2] for i in range(total_seed)], axis=0)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Number of experiments': range(1, len(random_nx)+1),\n",
    "    'Random': random_nx,\n",
    "    'Warm': warm_nx,\n",
    "    'Ideal': ideal_nx,\n",
    "    'Random_std': random_nx_std,\n",
    "    'Warm_std': warm_nx_std,\n",
    "    'Ideal_std': ideal_nx_std\n",
    "})\n",
    "\n",
    "data_melted = pd.melt(data, id_vars='Number of experiments', value_vars=['Random', 'Warm', 'Ideal'], var_name='Method', value_name='Number of data points w/ non-zero gradient')\n",
    "data_melted_std = pd.melt(data, id_vars='Number of experiments', value_vars=['Random_std', 'Warm_std', 'Ideal_std'], var_name='Method', value_name='Standard deviation')\n",
    "\n",
    "sns.lineplot(data=data_melted, x='Number of experiments', y='Number of data points w/ non-zero gradient', hue='Method', ax = ax[2], legend=False, palette=['green', 'navy', 'red'])\n",
    "\n",
    "for method, color in zip(['Random', 'Warm', 'Ideal'], ['green', 'navy', 'red']) :\n",
    "    ax[2].fill_between(data['Number of experiments'],\n",
    "                     data_melted[data_melted['Method'] == method]['Number of data points w/ non-zero gradient'] - data_melted_std[data_melted_std['Method'] == method+'_std']['Standard deviation'],\n",
    "                     data_melted[data_melted['Method'] == method]['Number of data points w/ non-zero gradient'] + data_melted_std[data_melted_std['Method'] == method+'_std']['Standard deviation'],\n",
    "                     alpha=0.1, color=color)\n",
    "\n",
    "ax[2].set_xlabel('Number of Experiments', fontsize=20)\n",
    "ax[2].set_ylabel('Number of Non-Zero Gradient Data', fontsize=15)\n",
    "\n",
    "\n",
    "random_f = np.mean(np.sum([np.sum(Learn_F[i, :, :, :, 0]==1, axis=1) for i in range(total_seed)], axis=-1), axis=0)\n",
    "warm_f = np.mean(np.sum([np.sum(Learn_F[i, :, :, :, 1]==1, axis=1) for i in range(total_seed)], axis=-1), axis=0)\n",
    "ideal_f = np.mean(np.sum([np.sum(Learn_F[i, :, :, :, 2]==1, axis=1) for i in range(total_seed)], axis=-1), axis=0)\n",
    "\n",
    "random_f_std = np.std(np.sum([np.sum(Learn_F[i, :, :, :, 0]==1, axis=1) for i in range(total_seed)], axis=-1), axis=0)\n",
    "warm_f_std = np.std(np.sum([np.sum(Learn_F[i, :, :, :, 1]==1, axis=1) for i in range(total_seed)], axis=-1), axis=0)\n",
    "ideal_f_std = np.std(np.sum([np.sum(Learn_F[i, :, :, :, 2]==1, axis=1) for i in range(total_seed)], axis=-1), axis=0)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Number of experiments': range(1, len(random_f)+1),\n",
    "    'Random': random_f,\n",
    "    'Warm': warm_f,\n",
    "    'Ideal': ideal_f,\n",
    "    'Random_std': random_f_std,\n",
    "    'Warm_std': warm_f_std,\n",
    "    'Ideal_std': ideal_f_std\n",
    "})\n",
    "\n",
    "data_melted = pd.melt(data, id_vars='Number of experiments', value_vars=['Random', 'Warm', 'Ideal'], var_name='Method', value_name='Number of learned features')\n",
    "data_melted_std = pd.melt(data, id_vars='Number of experiments', value_vars=['Random_std', 'Warm_std', 'Ideal_std'], var_name='Method', value_name='Standard deviation')\n",
    "\n",
    "sns.lineplot(data=data_melted, x='Number of experiments', y='Number of learned features', hue='Method', ax=ax[1], legend=False, palette=['green', 'navy', 'red'])\n",
    "\n",
    "for method, color in zip(['Random', 'Warm', 'Ideal'], ['green', 'navy', 'red']) :\n",
    "    ax[1].fill_between(data['Number of experiments'],\n",
    "                     data_melted[data_melted['Method'] == method]['Number of learned features'] - data_melted_std[data_melted_std['Method'] == method+'_std']['Standard deviation'],\n",
    "                     data_melted[data_melted['Method'] == method]['Number of learned features'] + data_melted_std[data_melted_std['Method'] == method+'_std']['Standard deviation'],\n",
    "                      alpha=0.1, color=color)\n",
    "\n",
    "ax[1].set_xlabel('Number of Experiments', fontsize=20)\n",
    "ax[1].set_ylabel('Number of Total Learned Features', fontsize=15)\n",
    "data = pd.DataFrame({\n",
    "    'Number of experiments': range(1, len(random_f)+1),\n",
    "    'Random': np.mean(test_acc, axis=0)[:, 0] * 100,\n",
    "    'Warm': np.mean(test_acc, axis=0)[:, 1] * 100,\n",
    "    'Ideal': np.mean(test_acc, axis=0)[:, 2] * 100,\n",
    "    'Random_std': np.std(test_acc, axis=0)[:, 0] * 100,\n",
    "    'Warm_std': np.std(test_acc, axis=0)[:, 1] * 100,\n",
    "    'Ideal_std': np.std(test_acc, axis=0)[:, 2] * 100,\n",
    "})\n",
    "\n",
    "data_melted = pd.melt(data, id_vars='Number of experiments', value_vars=['Random', 'Warm', 'Ideal'], var_name='Method', value_name='Number of learned features')\n",
    "data_melted_std = pd.melt(data, id_vars='Number of experiments', value_vars=['Random_std', 'Warm_std', 'Ideal_std'], var_name='Method', value_name='Standard deviation')\n",
    "\n",
    "if i == 0:\n",
    "    g = sns.lineplot(data=data_melted, x='Number of experiments', y='Number of learned features', hue='Method', ax=ax[0], palette=['green', 'navy', 'red'])\n",
    "else:\n",
    "    sns.lineplot(data=data_melted, x='Number of experiments', y='Number of learned features', hue='Method', ax=ax[0], legend=False, palette=['green', 'navy', 'red'])\n",
    "for method, color in zip(['Random', 'Warm', 'Ideal'], ['green', 'navy', 'red']) :\n",
    "    ax[0].fill_between(data['Number of experiments'],\n",
    "                     data_melted[data_melted['Method'] == method]['Number of learned features'] - data_melted_std[data_melted_std['Method'] == method+'_std']['Standard deviation'],\n",
    "                     data_melted[data_melted['Method'] == method]['Number of learned features'] + data_melted_std[data_melted_std['Method'] == method+'_std']['Standard deviation'],\n",
    "                     alpha=0.1, color=color)\n",
    "\n",
    "\n",
    "ax[0].set_xlabel('Number of Experiments', fontsize=20)\n",
    "ax[0].set_ylabel('Test Accuracy (%)', fontsize=20)\n",
    "if i == 0:\n",
    "    g.legend_.set_title('')\n",
    "    handles, labels = ax[i, 0].get_legend_handles_labels()\n",
    "    ax[0].legend(handles, labels, fontsize=20)   \n",
    "\n",
    "plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
